\section{Related work}
Most retinal bionic vision implants utilize a stream from an external video camera and provide preprocessed images to the user. Being physically rather close to natural human sight, we can apply standard techniques of image preprocessing like rescaling, edge detection, Gaussian blur, and others. Several works \cite{AYTON20201383, humayun2009preliminary} that study bionic implant users states that increasing the contrast of objects and providing them simplified scenes increases their orientation skills and allows them to complete tasks more efficiently. 

With new implants being developed, scientists suppose increasing of the computational power of Video Processing Units so it possibly can hold additional tasks connected with more complex image preprocessing methods, starting with seam carving \cite{AlAtabany2010} and up to introducing semantic segmentation of the whole image \cite{HORNE2016113}. 

Most of the recent papers which tried to enhance bionic vision tried to apply different saliency detection algorithms and add this information to the image \cite{LI20171, LI201864, Parikh_2010, 6091267, aor12498}. Saliency information is defined as information about image pixels that differ from surroundings, so the human eye would typically focus on them. To obtain a saliency map, there exist deep learning models \cite{kmmerer2016deepgaze} that provide qualitative predictions. However, despite saliency maps being useful, they are \textit{purpose-agnostic} so they highlight the information no matter what the task is and therefore wouldn't help much to search certain objects. 

Several papers also introduced using depth information \cite{Perez-Yus_2017_ICCV, McCarthy_2014} to enhance the image and provide the user some additional context. Unfortunately, these papers focused on an outdoor environment where depth sensors indisputably would help locate dangerous objects. Still, very little information about indoor objects search and any tasks except the basic location are presented.

The idea of applying segmentation or object detection models to the video stream isn't new in this area, as several papers have tried to do this. The paper by Weiland et al. \cite{6345928} tried to adjust the object detection algorithm to facilitate objects' location on the empty white table. However, objects were pretty separated and located far from each other. The paper by Horne et al. \cite{HORNE2016113} also introduced semantic segmentation of the image, but they tried to segment all the images and highlight those areas which are important in terms of outdoor navigation. Besides that, in the recent work of Han et al. \cite{han2021deep} a rather close approach was presented but with a focus on the outdoor environment. 

Finally, the closest to our ideas Sanchez-Garcia et al. \cite{visapp19} work was published two years ago. This paper introduces fully convolutional networks to segment the objects in indoor scenes. Despite being very close to our work, the paper focuses on directly highlighting the objects considering they are already far from each other and do not involve the separation task of adjacent objects. The authors directly assign a constant brightness level to every object and do not scrutinize situations when objects are in front of each other or located very close. To further separate our work from this paper, we focus on small objects and everyday items that bionic implant users possibly want to find and locate inside the room or other indoor location.