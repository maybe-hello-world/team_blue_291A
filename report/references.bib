@inproceedings{ociddataset,
  author    = {Markus Suchi and
               Timothy Patten and
               David Fischinger and
               Markus Vincze},
  title     = {EasyLabel: {A} Semi-Automatic Pixel-wise Object Annotation Tool for
               Creating Robotic {RGB-D} Datasets},
  booktitle = {International Conference on Robotics and Automation, {ICRA} 2019,
               Montreal, QC, Canada, May 20-24, 2019},
  pages     = {6678--6684},
  year      = {2019},
  crossref  = {DBLP:conf/icra/2019},
  url       = {https://doi.org/10.1109/ICRA.2019.8793917},
  doi       = {10.1109/ICRA.2019.8793917},
  timestamp = {Tue, 13 Aug 2019 20:25:20 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icra/SuchiPFV19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{DBLP:conf/icra/2019,
  title     = {International Conference on Robotics and Automation, {ICRA} 2019,
               Montreal, QC, Canada, May 20-24, 2019},
  publisher = {{IEEE}},
  year      = {2019},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8780387},
  isbn      = {978-1-5386-6027-0},
  timestamp = {Tue, 13 Aug 2019 20:23:21 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icra/2019},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{kmmerer2016deepgaze,
    title={DeepGaze II: Reading fixations from deep features trained on object recognition},
    author={Matthias Kümmerer and Thomas S. A. Wallis and Matthias Bethge},
    year={2016},
    eprint={1610.01563},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@misc{gupta2019lvis,
      title={LVIS: A Dataset for Large Vocabulary Instance Segmentation}, 
      author={Agrim Gupta and Piotr Dollár and Ross Girshick},
      year={2019},
      eprint={1908.03195},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{kubale2004graph,
  title={Graph Colorings},
  author={Kubale, M. and Optymalizacja Dyskretna English and American Mathematical Society},
  isbn={9780821834589},
  lccn={2004046151},
  series={Contemporary mathematics (American Mathematical Society) v. 352},
  url={https://books.google.ru/books?id=fokbCAAAQBAJ},
  year={2004},
  publisher={American Mathematical Society}
}

@misc{han2021deep,
      title={Deep Learning--Based Scene Simplification for Bionic Vision}, 
      author={Nicole Han and Sudhanshu Srivastava and Aiwen Xu and Devi Klein and Michael Beyeler},
      year={2021},
      eprint={2102.00297},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article {Beyeler148015,
	author = {Beyeler, Michael and Boynton, Geoffrey M. and Fine, Ione and Rokem, Ariel},
	title = {pulse2percept: A Python-based simulation framework for bionic vision},
	elocation-id = {148015},
	year = {2017},
	doi = {10.1101/148015},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {By 2020 roughly 200 million people worldwide will suffer from photoreceptor diseases such as retinitis pigmentosa and age-related macular degeneration, and a variety of retinal sight restoration technologies are being developed to target these diseases. One technology, analogous to cochlear implants, uses a grid of electrodes to stimulate remaining retinal cells. Two brands of retinal prostheses are currently approved for implantation in patients with late stage photoreceptor disease. Clinical experience with these implants has made it apparent that the vision restored by these devices differs substantially from normal sight. To better understand the outcomes of this technology, we developed pulse2percept, an open-source Python implementation of a computational model that predicts the perceptual experience of retinal prosthesis patients across a wide range of implant configurations. A modular and extensible user interface exposes the different building blocks of the software, making it easy for users to simulate novel implants, stimuli, and retinal models. We hope that this library will contribute substantially to the field of medicine by providing a tool to accelerate the development of visual prostheses.},
	URL = {https://www.biorxiv.org/content/early/2017/07/10/148015},
	eprint = {https://www.biorxiv.org/content/early/2017/07/10/148015.full.pdf},
	journal = {bioRxiv}
}

@article{AYTON20201383,
title = {An update on retinal prostheses},
journal = {Clinical Neurophysiology},
volume = {131},
number = {6},
pages = {1383-1398},
year = {2020},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2019.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S1388245719313264},
author = {Lauren N. Ayton and Nick Barnes and Gislin Dagnelie and Takashi Fujikado and Georges Goetz and Ralf Hornig and Bryan W. Jones and Mahiul M.K. Muqit and Daniel L. Rathbun and Katarina Stingl and James D. Weiland and Matthew A. Petoe},
keywords = {Retinal prosthesis, Vision restoration, Retinal disease, Ophthalmology},
abstract = {Retinal prostheses are designed to restore a basic sense of sight to people with profound vision loss. They require a relatively intact posterior visual pathway (optic nerve, lateral geniculate nucleus and visual cortex). Retinal implants are options for people with severe stages of retinal degenerative disease such as retinitis pigmentosa and age-related macular degeneration. There have now been three regulatory-approved retinal prostheses. Over five hundred patients have been implanted globally over the past 15 years. Devices generally provide an improved ability to localize high-contrast objects, navigate, and perform basic orientation tasks. Adverse events have included conjunctival erosion, retinal detachment, loss of light perception, and the need for revision surgery, but are rare. There are also specific device risks, including overstimulation (which could cause damage to the retina) or delamination of implanted components, but these are very unlikely. Current challenges include how to improve visual acuity, enlarge the field-of-view, and reduce a complex visual scene to its most salient components through image processing. This review encompasses the work of over 40 individual research groups who have built devices, developed stimulation strategies, or investigated the basic physiology underpinning retinal prostheses. Current technologies are summarized, along with future challenges that face the field.}
}

@inproceedings{humayun2009preliminary,
  title={Preliminary 6 month results from the argus tm ii epiretinal prosthesis feasibility study},
  author={Humayun, Mark S and Dorn, Jessy D and Ahuja, Ashish K and Caspi, Avi and Filley, Eugene and Dagnelie, Gislin and Salzmann, Jo{\"e}l and Santos, Arturo and Duncan, Jacque and Mohand-Said, Saddek and others},
  booktitle={2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
  pages={4566--4568},
  year={2009},
  organization={IEEE}
}

@Article{AlAtabany2010,
  author={Al-Atabany, Walid I.
  and Tong, Tzyy
  and Degenaar, Patrick A.},
  title={Improved content aware scene retargeting for retinitis pigmentosa patients},
  journal={BioMedical Engineering OnLine},
  year={2010},
  month={Sep},
  day={16},
  volume={9},
  number={1},
  pages={52},
  abstract={In this paper we present a novel scene retargeting technique to reduce the visual scene while maintaining the size of the key features. The algorithm is scalable to implementation onto portable devices, and thus, has potential for augmented reality systems to provide visual support for those with tunnel vision. We therefore test the efficacy of our algorithm on shrinking the visual scene into the remaining field of view for those patients.},
  issn={1475-925X},
  doi={10.1186/1475-925X-9-52},
  url={https://doi.org/10.1186/1475-925X-9-52}
}

@article{HORNE2016113,
title = {Semantic labeling for prosthetic vision},
journal = {Computer Vision and Image Understanding},
volume = {149},
pages = {113-125},
year = {2016},
note = {Special issue on Assistive Computer Vision and Robotics - "Assistive Solutions for Mobility, Communication and HMI"},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S1077314216000692},
author = {Lachlan Horne and Jose Alvarez and Chris McCarthy and Mathieu Salzmann and Nick Barnes},
keywords = {Prosthetic vision, Egocentic vision, Semantic labeling},
abstract = {Current and near-term implantable prosthetic vision systems offer the potential to restore some visual function, but suffer from limited resolution and dynamic range of induced visual percepts. This can make navigating complex environments difficult for users. We introduce semantic labeling as a technique to improve navigation outcomes for prosthetic vision users. We produce a novel egocentric vision dataset to demonstrate how semantic labeling can be applied to this problem. We also improve the speed of semantic labeling with sparse computation of unary potentials, enabling its use in real-time wearable assistive devices. We use simulated prosthetic vision to demonstrate the results of our technique. Our approach allows a prosthetic vision system to selectively highlight specific classes of objects in the user’s field of view, improving the user’s situational awareness.}
}

@article{LI20171,
title = {A real-time image optimization strategy based on global saliency detection for artificial retinal prostheses},
journal = {Information Sciences},
volume = {415-416},
pages = {1-18},
year = {2017},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516317571},
author = {Heng Li and Tingting Han and Jing Wang and Zhuofan Lu and Xiaofei Cao and Yao Chen and Liming Li and Chuanqing Zhou and Xinyu Chai},
keywords = {Retinal prostheses, Simulated prosthetic vision, Saliency detection, Eye-hand coordination},
abstract = {Current retinal prostheses can only generate low-resolution visual percepts constituted of inadequate phosphenes which are elicited by a limited number of stimulating electrodes and with unruly color and restricted grayscale. Fortunately, for most retinal prostheses, an external camera and a video processing unit are employed to be essential components, and allow image processing to improve visual perception for recipients. At present, there have been some studies that use a variety of sophisticated image processing algorithms to improve prosthetic vision perception. However, most of them cannot achieve real-time processing due to the complexity of the algorithms and the limitation of platform processing power. This greatly curbs the practical application of these algorithms on the retinal prostheses. In this study, we propose a real-time image processing strategy based on a novel bottom-up saliency detection algorithm, aiming to detect and enhance foreground objects in a scene. Results demonstrate by verification of conducting two eye-hand-coordination visual tasks that under simulated prosthetic vision, our proposed strategy has noticeable advantages in terms of accuracy, efficiency, and head motion range. The study aims to help develop image processing modules in retinal prostheses, and is hoped to provide more benefit towards prosthesis recipients.}
}

@article{LI201864,
title = {Image processing strategies based on saliency segmentation for object recognition under simulated prosthetic vision},
journal = {Artificial Intelligence in Medicine},
volume = {84},
pages = {64-78},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365716304195},
author = {Heng Li and Xiaofan Su and Jing Wang and Han Kan and Tingting Han and Yajie Zeng and Xinyu Chai},
keywords = {Visual prosthesis, Simulated prosthetic vision, Saliency segmentation, Image processing strategy, Objects recognition},
abstract = {Background and objective
Current retinal prostheses can only generate low-resolution visual percepts constituted of limited phosphenes which are elicited by an electrode array and with uncontrollable color and restricted grayscale. Under this visual perception, prosthetic recipients can just complete some simple visual tasks, but more complex tasks like face identification/object recognition are extremely difficult. Therefore, it is necessary to investigate and apply image processing strategies for optimizing the visual perception of the recipients. This study focuses on recognition of the object of interest employing simulated prosthetic vision.
Method
We used a saliency segmentation method based on a biologically plausible graph-based visual saliency model and a grabCut-based self-adaptive-iterative optimization framework to automatically extract foreground objects. Based on this, two image processing strategies, Addition of Separate Pixelization and Background Pixel Shrink, were further utilized to enhance the extracted foreground objects.
Results
i) The results showed by verification of psychophysical experiments that under simulated prosthetic vision, both strategies had marked advantages over Direct Pixelization in terms of recognition accuracy and efficiency. ii) We also found that recognition performance under two strategies was tied to the segmentation results and was affected positively by the paired-interrelated objects in the scene.
Conclusion
The use of the saliency segmentation method and image processing strategies can automatically extract and enhance foreground objects, and significantly improve object recognition performance towards recipients implanted a high-density implant.}
}

@article{Parikh_2010,
	doi = {10.1088/1741-2560/7/1/016006},
	url = {https://doi.org/10.1088/1741-2560/7/1/016006},
	year = 2010,
	month = {jan},
	publisher = {{IOP} Publishing},
	volume = {7},
	number = {1},
	pages = {016006},
	author = {N Parikh and L Itti and J Weiland},
	title = {Saliency-based image processing for retinal prostheses},
	journal = {Journal of Neural Engineering},
	abstract = {We present a computationally efficient model for detecting salient regions in an image frame. The model when implemented on a portable, wearable system can be used in conjunction with a retinal prosthesis, to identify important objects that a retinal prosthesis patient may not be able to see due to implant limitations. The model is based on an earlier saliency detection model but has a reduced number of parallel streams. Results of a comparison between the areas detected as salient by the algorithm and areas gazed at by human subjects in a set of images show a correspondence which is greater than what would be expected by chance. Initial results for a comparison of the execution speed of the two algorithm models for each frame on the TMS320 DM642 Texas Instruments Digital Signal Processor suggest that the proposed model is approximately ten times faster than the original saliency model.}
}

@INPROCEEDINGS{6091267,

  author={Stacey, Ashley and Li, Yi and Barnes, Nick},

  booktitle={2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 

  title={A salient information processing system for bionic eye with application to obstacle avoidance}, 

  year={2011},

  volume={},

  number={},

  pages={5116-5119},

  doi={10.1109/IEMBS.2011.6091267}}

@article{aor12498,
author = {Wang, Jing and Li, Heng and Fu, Weizhen and Chen, Yao and Li, Liming and Lyu, Qing and Han, Tingting and Chai, Xinyu},
title = {Image Processing Strategies Based on a Visual Saliency Model for Object Recognition Under Simulated Prosthetic Vision},
journal = {Artificial Organs},
volume = {40},
number = {1},
pages = {94-100},
keywords = {Retinal prosthesis, Simulated prosthetic vision, Object recognition, Visual saliency, Image processing},
doi = {https://doi.org/10.1111/aor.12498},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/aor.12498},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/aor.12498},
abstract = {Abstract Retinal prostheses have the potential to restore partial vision. Object recognition in scenes of daily life is one of the essential tasks for implant wearers. Still limited by the low-resolution visual percepts provided by retinal prostheses, it is important to investigate and apply image processing methods to convey more useful visual information to the wearers. We proposed two image processing strategies based on Itti's visual saliency map, region of interest (ROI) extraction, and image segmentation. Itti's saliency model generated a saliency map from the original image, in which salient regions were grouped into ROI by the fuzzy c-means clustering. Then Grabcut generated a proto-object from the ROI labeled image which was recombined with background and enhanced in two ways—8-4 separated pixelization (8-4 SP) and background edge extraction (BEE). Results showed that both 8-4 SP and BEE had significantly higher recognition accuracy in comparison with direct pixelization (DP). Each saliency-based image processing strategy was subject to the performance of image segmentation. Under good and perfect segmentation conditions, BEE and 8-4 SP obtained noticeably higher recognition accuracy than DP, and under bad segmentation condition, only BEE boosted the performance. The application of saliency-based image processing strategies was verified to be beneficial to object recognition in daily scenes under simulated prosthetic vision. They are hoped to help the development of the image processing module for future retinal prostheses, and thus provide more benefit for the patients.},
year = {2016}
}

@InProceedings{Perez-Yus_2017_ICCV,
author = {Perez-Yus, Alejandro and Bermudez-Cameo, Jesus and Lopez-Nicolas, Gonzalo and Guerrero, Jose J.},
title = {Depth and Motion Cues With Phosphene Patterns for Prosthetic Vision},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2017}
} 

@article{McCarthy_2014,
	doi = {10.1088/1741-2560/12/1/016003},
	url = {https://doi.org/10.1088/1741-2560/12/1/016003},
	year = 2014,
	month = {nov},
	publisher = {{IOP} Publishing},
	volume = {12},
	number = {1},
	pages = {016003},
	author = {Chris McCarthy and Janine G Walker and Paulette Lieby and Adele Scott and Nick Barnes},
	title = {Mobility and low contrast trip hazard avoidance using augmented depth},
	journal = {Journal of Neural Engineering},
	abstract = {Objective. We evaluated a novel visual representation for current and near-term prosthetic vision. Augmented depth emphasizes ground obstacles and floor-wall boundaries in a depth-based visual representation. This is achieved by artificially increasing contrast between obstacles and the ground surface via a novel ground plane extraction algorithm specifically designed to preserve low-contrast ground-surface boundaries. Approach. The effectiveness of augmented depth was examined in human mobility trials compared against standard intensity-based (Intensity), depth-based (Depth) and random (Random) visual representations. Eight participants with normal vision used simulated prosthetic vision with 20 phosphenes and eight perceivable brightness levels to traverse a course with randomly placed small and low-contrast obstacles on the ground. Main results. The number of collisions was significantly reduced using augmented depth, compared with intensity, depth and random representations (48%, 44% and 72% less collisions, respectively). Significance. These results indicate that augmented depth may enable safe mobility in the presence of low-contrast obstacles with current and near-term implants. This is the first demonstration that an augmentation of the scene ensuring key objects are visible may provide better outcomes for prosthetic vision.}
}

@INPROCEEDINGS{6345928,

  author={Weiland, James D. and Parikh, Neha and Pradeep, Vivek and Medioni, Gerard},

  booktitle={2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 

  title={Smart image processing system for retinal prosthesis}, 

  year={2012},

  volume={},

  number={},

  pages={300-303},

  doi={10.1109/EMBC.2012.6345928}}

  @conference{visapp19,
author={Melani Sanchez{-}Garcia. and Ruben Martinez{-}Cantin. and Jose Guerrero.},
title={Indoor Scenes Understanding for Visual Prosthesis with Fully Convolutional Networks},
booktitle={Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 5: VISAPP,},
year={2019},
pages={218-225},
publisher={SciTePress},
organization={INSTICC},
doi={10.5220/0007257602180225},
isbn={978-989-758-354-4},
issn={2184-4321},
}